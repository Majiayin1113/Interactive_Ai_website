import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration
import cv2
import numpy as np
import av
from typing import Union

# è¯­è¨€æ–‡æœ¬å­—å…¸
CAMERA_LANGUAGES = {
    'zh': {
        'page_title': 'AIæ‘„åƒå¤´äººè„¸è¯†åˆ«',
        'app_title': 'ğŸ“¹ AIæ‘„åƒå¤´äººè„¸è¯†åˆ«',
        'camera_settings': 'ğŸ“¹ æ‘„åƒå¤´è®¾ç½®',
        'detection_settings': 'ğŸ” æ£€æµ‹è®¾ç½®',
        'face_detection': 'äººè„¸æ£€æµ‹',
        'face_detection_help': 'å¼€å¯åå°†åœ¨è§†é¢‘ä¸­æ ‡è®°æ£€æµ‹åˆ°çš„äººè„¸',
        'confidence_threshold': 'æ£€æµ‹ç½®ä¿¡åº¦',
        'confidence_help': 'ç½®ä¿¡åº¦è¶Šé«˜ï¼Œæ£€æµ‹è¶Šä¸¥æ ¼',
        'detection_color': 'æ£€æµ‹æ¡†é¢œè‰²',
        'start_camera': 'ğŸ“¹ å¯åŠ¨æ‘„åƒå¤´',
        'stop_camera': 'â¹ï¸ åœæ­¢æ‘„åƒå¤´',
        'camera_status': 'ğŸ“Š æ‘„åƒå¤´çŠ¶æ€',
        'status_ready': 'ğŸŸ¢ å°±ç»ª',
        'status_running': 'ğŸŸ¡ è¿è¡Œä¸­',
        'status_stopped': 'ğŸ”´ å·²åœæ­¢',
        'detection_stats': 'ğŸ“ˆ æ£€æµ‹ç»Ÿè®¡',
        'faces_detected': 'æ£€æµ‹åˆ°çš„äººè„¸æ•°é‡',
        'processing_fps': 'å¤„ç†å¸§ç‡',
        'instructions_title': 'ğŸ“‹ ä½¿ç”¨è¯´æ˜',
        'instructions_1': '1. ç‚¹å‡»"å¯åŠ¨æ‘„åƒå¤´"å¼€å§‹è§†é¢‘æµ',
        'instructions_2': '2. è°ƒæ•´æ£€æµ‹è®¾ç½®æ¥ä¼˜åŒ–è¯†åˆ«æ•ˆæœ',
        'instructions_3': '3. ç»¿è‰²æ¡†è¡¨ç¤ºæ£€æµ‹åˆ°çš„äººè„¸',
        'instructions_4': '4. å¯ä»¥è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼æ¥æ”¹å˜æ£€æµ‹çµæ•åº¦',
        'privacy_notice': 'ğŸ”’ éšç§æé†’ï¼šè§†é¢‘æµä»…åœ¨æœ¬åœ°å¤„ç†ï¼Œä¸ä¼šä¸Šä¼ åˆ°æœåŠ¡å™¨',
        'language_switch': 'ğŸŒ Language',
        'lang_zh': 'ğŸ‡¨ğŸ‡³ ä¸­æ–‡',
        'lang_en': 'ğŸ‡ºğŸ‡¸ English'
    },
    'en': {
        'page_title': 'AI Camera Face Detection',
        'app_title': 'ğŸ“¹ AI Camera Face Detection',
        'camera_settings': 'ğŸ“¹ Camera Settings',
        'detection_settings': 'ğŸ” Detection Settings',
        'face_detection': 'Face Detection',
        'face_detection_help': 'Enable to mark detected faces in video',
        'confidence_threshold': 'Detection Confidence',
        'confidence_help': 'Higher confidence means stricter detection',
        'detection_color': 'Detection Box Color',
        'start_camera': 'ğŸ“¹ Start Camera',
        'stop_camera': 'â¹ï¸ Stop Camera',
        'camera_status': 'ğŸ“Š Camera Status',
        'status_ready': 'ğŸŸ¢ Ready',
        'status_running': 'ğŸŸ¡ Running',
        'status_stopped': 'ğŸ”´ Stopped',
        'detection_stats': 'ğŸ“ˆ Detection Stats',
        'faces_detected': 'Faces Detected',
        'processing_fps': 'Processing FPS',
        'instructions_title': 'ğŸ“‹ Instructions',
        'instructions_1': '1. Click "Start Camera" to begin video stream',
        'instructions_2': '2. Adjust detection settings to optimize recognition',
        'instructions_3': '3. Green boxes indicate detected faces',
        'instructions_4': '4. Adjust confidence threshold to change sensitivity',
        'privacy_notice': 'ğŸ”’ Privacy Notice: Video stream is processed locally only, not uploaded to server',
        'language_switch': 'ğŸŒ è¯­è¨€',
        'lang_zh': 'ğŸ‡¨ğŸ‡³ ä¸­æ–‡',
        'lang_en': 'ğŸ‡ºğŸ‡¸ English'
    }
}

# è·å–å½“å‰è¯­è¨€æ–‡æœ¬
def get_camera_text(key, lang='zh'):
    return CAMERA_LANGUAGES.get(lang, CAMERA_LANGUAGES['zh']).get(key, key)

# åˆå§‹åŒ–è¯­è¨€è®¾ç½®
if 'camera_language' not in st.session_state:
    st.session_state.camera_language = 'zh'

# åˆå§‹åŒ–æ£€æµ‹ç»Ÿè®¡
if 'face_count' not in st.session_state:
    st.session_state.face_count = 0
if 'processing_fps' not in st.session_state:
    st.session_state.processing_fps = 0

class FaceDetectionTransformer(VideoTransformerBase):
    def __init__(self):
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        self.confidence_threshold = 0.3
        self.detection_color = (0, 255, 0)  # Green
        self.face_detection_enabled = True
        self.frame_count = 0
        self.last_fps_time = cv2.getTickCount()
        
    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        
        if self.face_detection_enabled:
            # è½¬æ¢ä¸ºç°åº¦å›¾è¿›è¡Œäººè„¸æ£€æµ‹
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            
            # æ£€æµ‹äººè„¸
            faces = self.face_cascade.detectMultiScale(
                gray,
                scaleFactor=1.1,
                minNeighbors=5,
                minSize=(30, 30),
                flags=cv2.CASCADE_SCALE_IMAGE
            )
            
            # æ›´æ–°äººè„¸æ•°é‡ç»Ÿè®¡
            st.session_state.face_count = len(faces)
            
            # ç»˜åˆ¶äººè„¸æ¡†
            for (x, y, w, h) in faces:
                cv2.rectangle(img, (x, y), (x + w, y + h), self.detection_color, 2)
                
                # æ·»åŠ ç½®ä¿¡åº¦æ–‡æœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰
                cv2.putText(img, 'Face', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.detection_color, 1)
        
        # è®¡ç®—FPS
        self.frame_count += 1
        if self.frame_count % 30 == 0:  # æ¯30å¸§è®¡ç®—ä¸€æ¬¡FPS
            current_time = cv2.getTickCount()
            fps = 30.0 / ((current_time - self.last_fps_time) / cv2.getTickFrequency())
            st.session_state.processing_fps = round(fps, 1)
            self.last_fps_time = current_time
        
        return av.VideoFrame.from_ndarray(img, format="bgr24")
    
    def update_settings(self, confidence_threshold, detection_color, face_detection_enabled):
        self.confidence_threshold = confidence_threshold
        self.detection_color = detection_color
        self.face_detection_enabled = face_detection_enabled

# WebRTCé…ç½®
RTC_CONFIGURATION = RTCConfiguration({
    "iceServers": [
        {"urls": ["stun:stun.l.google.com:19302"]},
    ]
})

def main():
    # é¡µé¢é…ç½®
    st.set_page_config(
        page_title=get_camera_text('page_title', st.session_state.camera_language),
        page_icon="ğŸ“¹",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # å·¦ä¾§è¾¹æ 
    with st.sidebar:
        # è¯­è¨€åˆ‡æ¢æŒ‰é’®
        st.subheader(get_camera_text('language_switch', st.session_state.camera_language))
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button(get_camera_text('lang_zh', st.session_state.camera_language), use_container_width=True):
                st.session_state.camera_language = 'zh'
                st.rerun()
                
        with col2:
            if st.button(get_camera_text('lang_en', st.session_state.camera_language), use_container_width=True):
                st.session_state.camera_language = 'en'
                st.rerun()
        
        st.markdown("---")
        
        st.title(get_camera_text('app_title', st.session_state.camera_language))
        st.markdown("---")
        
        # æ‘„åƒå¤´è®¾ç½®
        st.subheader(get_camera_text('camera_settings', st.session_state.camera_language))
        
        # äººè„¸æ£€æµ‹å¼€å…³
        face_detection_enabled = st.checkbox(
            get_camera_text('face_detection', st.session_state.camera_language),
            value=True,
            help=get_camera_text('face_detection_help', st.session_state.camera_language)
        )
        
        st.markdown("---")
        
        # æ£€æµ‹è®¾ç½®
        st.subheader(get_camera_text('detection_settings', st.session_state.camera_language))
        
        # ç½®ä¿¡åº¦é˜ˆå€¼
        confidence_threshold = st.slider(
            get_camera_text('confidence_threshold', st.session_state.camera_language),
            min_value=0.1,
            max_value=1.0,
            value=0.3,
            step=0.05,
            help=get_camera_text('confidence_help', st.session_state.camera_language)
        )
        
        # æ£€æµ‹æ¡†é¢œè‰²é€‰æ‹©
        st.write(get_camera_text('detection_color', st.session_state.camera_language))
        color_option = st.selectbox(
            "",
            ["Green", "Red", "Blue", "Yellow", "Purple"],
            index=0
        )
        
        color_map = {
            "Green": (0, 255, 0),
            "Red": (0, 0, 255),
            "Blue": (255, 0, 0),
            "Yellow": (0, 255, 255),
            "Purple": (255, 0, 255)
        }
        detection_color = color_map[color_option]
        
        st.markdown("---")
        
        # æ‘„åƒå¤´çŠ¶æ€
        st.subheader(get_camera_text('camera_status', st.session_state.camera_language))
        
        if 'webrtc_ctx' in st.session_state and st.session_state.webrtc_ctx:
            if st.session_state.webrtc_ctx.state.playing:
                st.success(get_camera_text('status_running', st.session_state.camera_language))
            else:
                st.info(get_camera_text('status_ready', st.session_state.camera_language))
        else:
            st.warning(get_camera_text('status_stopped', st.session_state.camera_language))
        
        st.markdown("---")
        
        # æ£€æµ‹ç»Ÿè®¡
        st.subheader(get_camera_text('detection_stats', st.session_state.camera_language))
        
        col_faces, col_fps = st.columns(2)
        with col_faces:
            st.metric(
                get_camera_text('faces_detected', st.session_state.camera_language),
                st.session_state.face_count
            )
        
        with col_fps:
            st.metric(
                get_camera_text('processing_fps', st.session_state.camera_language),
                f"{st.session_state.processing_fps}"
            )
    
    # ä¸»å†…å®¹åŒºåŸŸ
    st.title(get_camera_text('app_title', st.session_state.camera_language))
    
    # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # WebRTCæ‘„åƒå¤´æµ
        webrtc_ctx = webrtc_streamer(
            key="face-detection",
            mode="sendrecv",
            rtc_configuration=RTC_CONFIGURATION,
            video_processor_factory=FaceDetectionTransformer,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
        )
        
        # å­˜å‚¨webrtc contextåˆ°session state
        st.session_state.webrtc_ctx = webrtc_ctx
        
        # æ›´æ–°transformerè®¾ç½®
        if webrtc_ctx.video_processor:
            webrtc_ctx.video_processor.update_settings(
                confidence_threshold,
                detection_color,
                face_detection_enabled
            )
    
    with col2:
        # ä½¿ç”¨è¯´æ˜
        st.subheader(get_camera_text('instructions_title', st.session_state.camera_language))
        
        st.markdown(f"""
        {get_camera_text('instructions_1', st.session_state.camera_language)}
        
        {get_camera_text('instructions_2', st.session_state.camera_language)}
        
        {get_camera_text('instructions_3', st.session_state.camera_language)}
        
        {get_camera_text('instructions_4', st.session_state.camera_language)}
        """)
        
        st.info(get_camera_text('privacy_notice', st.session_state.camera_language))
        
        # å®æ—¶ç»Ÿè®¡æ˜¾ç¤º
        if face_detection_enabled:
            st.success(f"âœ… {get_camera_text('face_detection', st.session_state.camera_language)}: ON")
        else:
            st.warning(f"âš ï¸ {get_camera_text('face_detection', st.session_state.camera_language)}: OFF")

if __name__ == "__main__":
    main()
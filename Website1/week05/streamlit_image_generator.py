import streamlit as st
from diffusers import DiffusionPipeline
import torch
from io import BytesIO
import time
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration, WebRtcMode
import cv2
import numpy as np
import av

# è¯­è¨€æ–‡æœ¬å­—å…¸
LANGUAGES = {
    'zh': {
        'page_title': 'AIæ™ºèƒ½å·¥å…·é›†',
        'app_title': 'ğŸ¨ AIå›¾åƒç”Ÿæˆå™¨',
        'system_info': 'âš™ï¸ ç³»ç»Ÿä¿¡æ¯',
        'pytorch_info': f'ğŸ”§ PyTorch: {torch.__version__}',
        'device_gpu': 'ğŸ–¥ï¸ è®¾å¤‡: GPU (CUDA)',
        'device_cpu': 'ğŸ–¥ï¸ è®¾å¤‡: CPU',
        'image_settings': 'ğŸ“ å›¾åƒè®¾ç½®',
        'image_description': 'å›¾åƒæè¿°',
        'image_description_help': 'è¯¦ç»†æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„å›¾åƒ',
        'default_prompt': 'a beautiful landscape with mountains',
        'generation_steps': 'ç”Ÿæˆæ­¥æ•°',
        'steps_help': 'æ›´é«˜çš„æ­¥æ•°é€šå¸¸äº§ç”Ÿæ›´å¥½çš„è´¨é‡ï¼Œä½†éœ€è¦æ›´é•¿æ—¶é—´',
        'image_dimensions': 'ğŸ“ å›¾åƒå°ºå¯¸',
        'select_size': 'é€‰æ‹©å°ºå¯¸',
        'size_512x512': '512x512 (æ ‡å‡†)',
        'size_768x512': '768x512 (å®½å±)',
        'size_512x768': '512x768 (ç«–å±)',
        'quick_select': 'ğŸ¯ å¿«é€Ÿé€‰æ‹©',
        'preset_puppy': 'ğŸ• å¯çˆ±å°ç‹—',
        'preset_landscape': 'ğŸ”ï¸ ç¾ä¸½é£æ™¯',
        'preset_city': 'ğŸŒƒ æœªæ¥åŸå¸‚',
        'preset_forest': 'ğŸŒ² æ¢¦å¹»æ£®æ—',
        'preset_abstract': 'ğŸ¨ æŠ½è±¡è‰ºæœ¯',
        'preset_sakura': 'ğŸŒ¸ æ¨±èŠ±å­£èŠ‚',
        'generate_button': 'ğŸ¨ ç”Ÿæˆå›¾åƒ',
        'image_display': 'ğŸ–¼ï¸ å›¾åƒå±•ç¤ºåŒºåŸŸ',
        'generating': 'ğŸ¨ æ­£åœ¨ç”Ÿæˆå›¾åƒï¼Œè¯·ç¨å€™...',
        'loading_model': 'ğŸ“¥ åŠ è½½æ¨¡å‹...',
        'generating_image': 'ğŸ¨ ç”Ÿæˆå›¾åƒä¸­...',
        'generation_complete': 'âœ… ç”Ÿæˆå®Œæˆï¼',
        'generation_success': 'ğŸ‰ å›¾åƒç”ŸæˆæˆåŠŸï¼',
        'generation_failed': 'âŒ ç”Ÿæˆå¤±è´¥: ',
        'latest_image': 'ğŸ“¸ æœ€æ–°ç”Ÿæˆçš„å›¾åƒ',
        'prompt_label': 'æç¤ºè¯: ',
        'prompt_info': 'ğŸ“ **æç¤ºè¯**: ',
        'steps_info': 'ğŸ”¢ **ç”Ÿæˆæ­¥æ•°**: ',
        'size_info': 'ğŸ“ **å°ºå¯¸**: ',
        'time_info': 'ğŸ• **ç”Ÿæˆæ—¶é—´**: ',
        'download_button': 'ğŸ“¥ ä¸‹è½½é«˜æ¸…å›¾åƒ',
        'regenerate_button': 'ğŸ”„ é‡æ–°ç”Ÿæˆ',
        'clear_history': 'ğŸ—‘ï¸ æ¸…é™¤å†å²',
        'history_title': 'ğŸ“š å†å²ç”Ÿæˆè®°å½•',
        'view_details': 'æŸ¥çœ‹è¯¦æƒ… #',
        'empty_state': 'ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾¹æ ä¸­è¾“å…¥æç¤ºè¯å¹¶ç‚¹å‡»ç”ŸæˆæŒ‰é’®å¼€å§‹åˆ›ä½œï¼',
        'features_title': 'âœ¨ åŠŸèƒ½ç‰¹è‰²',
        'feature_quality': '### ğŸ¨ é«˜è´¨é‡ç”Ÿæˆ\n- ä½¿ç”¨Stable Diffusion v1.5\n- æ”¯æŒå¤šç§å›¾åƒå°ºå¯¸\n- å¯è°ƒèŠ‚ç”Ÿæˆæ­¥æ•°',
        'feature_gpu': '### ğŸš€ GPUåŠ é€Ÿ\n- è‡ªåŠ¨æ£€æµ‹CUDAæ”¯æŒ\n- å¿«é€Ÿç”Ÿæˆå“åº”\n- ä¼˜åŒ–å†…å­˜ä½¿ç”¨',
        'feature_ui': '### ğŸ“± å‹å¥½ç•Œé¢\n- é¢„è®¾æç¤ºè¯é€‰æ‹©\n- å®æ—¶è¿›åº¦æ˜¾ç¤º\n- å†å²è®°å½•ç®¡ç†',
        'language_switch': 'ğŸŒ Language',
        'lang_zh': 'ğŸ‡¨ğŸ‡³ ä¸­æ–‡',
        'lang_en': 'ğŸ‡ºğŸ‡¸ English',
        'navigation': 'ğŸ§­ å¯¼èˆª',
        'page_image_gen': 'ğŸ¨ å›¾åƒç”Ÿæˆå™¨',
        'page_camera': 'ğŸ“¹ æ‘„åƒå¤´è¯†åˆ«',
        'nav_image_gen': 'å›¾åƒç”Ÿæˆ',
        'nav_camera': 'äººè„¸è¯†åˆ«'
    },
    'en': {
        'page_title': 'AI Smart Tools',
        'app_title': 'ğŸ¨ AI Image Generator',
        'system_info': 'âš™ï¸ System Info',
        'pytorch_info': f'ğŸ”§ PyTorch: {torch.__version__}',
        'device_gpu': 'ğŸ–¥ï¸ Device: GPU (CUDA)',
        'device_cpu': 'ğŸ–¥ï¸ Device: CPU',
        'image_settings': 'ğŸ“ Image Settings',
        'image_description': 'Image Description',
        'image_description_help': 'Describe the image you want to generate in detail',
        'default_prompt': 'a beautiful landscape with mountains',
        'generation_steps': 'Generation Steps',
        'steps_help': 'Higher steps usually produce better quality but take longer',
        'image_dimensions': 'ğŸ“ Image Dimensions',
        'select_size': 'Select Size',
        'size_512x512': '512x512 (Standard)',
        'size_768x512': '768x512 (Widescreen)',
        'size_512x768': '512x768 (Portrait)',
        'quick_select': 'ğŸ¯ Quick Select',
        'preset_puppy': 'ğŸ• Cute Puppy',
        'preset_landscape': 'ğŸ”ï¸ Beautiful Landscape',
        'preset_city': 'ğŸŒƒ Futuristic City',
        'preset_forest': 'ğŸŒ² Magical Forest',
        'preset_abstract': 'ğŸ¨ Abstract Art',
        'preset_sakura': 'ğŸŒ¸ Cherry Blossoms',
        'generate_button': 'ğŸ¨ Generate Image',
        'image_display': 'ğŸ–¼ï¸ Image Display Area',
        'generating': 'ğŸ¨ Generating image, please wait...',
        'loading_model': 'ğŸ“¥ Loading model...',
        'generating_image': 'ğŸ¨ Generating image...',
        'generation_complete': 'âœ… Generation complete!',
        'generation_success': 'ğŸ‰ Image generated successfully!',
        'generation_failed': 'âŒ Generation failed: ',
        'latest_image': 'ğŸ“¸ Latest Generated Image',
        'prompt_label': 'Prompt: ',
        'prompt_info': 'ğŸ“ **Prompt**: ',
        'steps_info': 'ğŸ”¢ **Steps**: ',
        'size_info': 'ğŸ“ **Size**: ',
        'time_info': 'ğŸ• **Generated**: ',
        'download_button': 'ğŸ“¥ Download HD Image',
        'regenerate_button': 'ğŸ”„ Regenerate',
        'clear_history': 'ğŸ—‘ï¸ Clear History',
        'history_title': 'ğŸ“š Generation History',
        'view_details': 'View Details #',
        'empty_state': 'ğŸ‘ˆ Please enter a prompt in the left sidebar and click generate to start creating!',
        'features_title': 'âœ¨ Features',
        'feature_quality': '### ğŸ¨ High Quality Generation\n- Uses Stable Diffusion v1.5\n- Multiple image sizes supported\n- Adjustable generation steps',
        'feature_gpu': '### ğŸš€ GPU Acceleration\n- Auto-detect CUDA support\n- Fast generation response\n- Optimized memory usage',
        'feature_ui': '### ğŸ“± User-Friendly Interface\n- Preset prompt selection\n- Real-time progress display\n- History management',
        'language_switch': 'ğŸŒ è¯­è¨€',
        'lang_zh': 'ğŸ‡¨ğŸ‡³ ä¸­æ–‡',
        'lang_en': 'ğŸ‡ºğŸ‡¸ English',
        'navigation': 'ğŸ§­ Navigation',
        'page_image_gen': 'ğŸ¨ Image Generator',
        'page_camera': 'ğŸ“¹ Camera Detection',
        'nav_image_gen': 'Image Generation',
        'nav_camera': 'Face Recognition'
    }
}

# é¢„è®¾æç¤ºè¯å­—å…¸
PRESET_PROMPTS = {
    'zh': {
        "ğŸ• å¯çˆ±å°ç‹—": "a cute little puppy, fluffy fur, adorable eyes, sitting on grass, high quality, detailed, photorealistic",
        "ğŸ”ï¸ ç¾ä¸½é£æ™¯": "a beautiful landscape with mountains, blue sky, green trees, peaceful lake, sunrise, detailed, photorealistic",
        "ğŸŒƒ æœªæ¥åŸå¸‚": "futuristic city, neon lights, flying cars, cyberpunk style, high tech buildings, night scene, detailed",
        "ğŸŒ² æ¢¦å¹»æ£®æ—": "magical forest, glowing mushrooms, fairy lights, mystical atmosphere, enchanted trees, fantasy art",
        "ğŸ¨ æŠ½è±¡è‰ºæœ¯": "abstract art, colorful, geometric shapes, modern art style, vibrant colors, artistic",
        "ğŸŒ¸ æ¨±èŠ±å­£èŠ‚": "cherry blossoms, sakura trees, pink petals falling, peaceful garden, spring season, detailed"
    },
    'en': {
        "ğŸ• Cute Puppy": "a cute little puppy, fluffy fur, adorable eyes, sitting on grass, high quality, detailed, photorealistic",
        "ğŸ”ï¸ Beautiful Landscape": "a beautiful landscape with mountains, blue sky, green trees, peaceful lake, sunrise, detailed, photorealistic",
        "ğŸŒƒ Futuristic City": "futuristic city, neon lights, flying cars, cyberpunk style, high tech buildings, night scene, detailed",
        "ğŸŒ² Magical Forest": "magical forest, glowing mushrooms, fairy lights, mystical atmosphere, enchanted trees, fantasy art",
        "ğŸ¨ Abstract Art": "abstract art, colorful, geometric shapes, modern art style, vibrant colors, artistic",
        "ğŸŒ¸ Cherry Blossoms": "cherry blossoms, sakura trees, pink petals falling, peaceful garden, spring season, detailed"
    }
}

# è·å–å½“å‰è¯­è¨€æ–‡æœ¬
def get_text(key, lang='zh'):
    return LANGUAGES.get(lang, LANGUAGES['zh']).get(key, key)

# åˆå§‹åŒ–è¯­è¨€è®¾ç½®
if 'language' not in st.session_state:
    st.session_state.language = 'zh'

# åˆå§‹åŒ–é¡µé¢é€‰æ‹©
if 'current_page' not in st.session_state:
    st.session_state.current_page = 'image_gen'

# åˆå§‹åŒ–æ‘„åƒå¤´ç›¸å…³çŠ¶æ€
if 'face_count' not in st.session_state:
    st.session_state.face_count = 0
if 'processing_fps' not in st.session_state:
    st.session_state.processing_fps = 0

# WebRTCé…ç½®
RTC_CONFIGURATION = RTCConfiguration({
    "iceServers": [
        {"urls": ["stun:stun.l.google.com:19302"]},
    ]
})

# å…¨å±€å˜é‡å­˜å‚¨è®¾ç½®
face_detection_settings = {
    'enabled': True,
    'color': (0, 255, 0),
    'confidence': 0.3,
    'falling_effect': True,
    'falling_speed': 3.0
}

# æ‰è½äººè„¸æ•°æ®ç»“æ„
falling_faces = []
last_face_capture_time = 0

class FallingFace:
    def __init__(self, face_img, x_start, frame_width, frame_height):
        self.face_img = face_img
        self.x = x_start + (face_img.shape[1] // 2)  # ä»äººè„¸ä¸­å¿ƒå¼€å§‹
        self.y = -face_img.shape[0]  # ä»é¡¶éƒ¨å¼€å§‹
        self.width = face_img.shape[1]
        self.height = face_img.shape[0]
        self.speed = face_detection_settings['falling_speed']
        self.frame_width = frame_width
        self.frame_height = frame_height
        self.rotation = 0
        import random
        self.rotation_speed = random.uniform(-2, 2)  # éšæœºæ—‹è½¬é€Ÿåº¦
        
    def update(self):
        self.y += self.speed
        self.rotation += self.rotation_speed
        return self.y < self.frame_height + 50  # è¶…å‡ºå±å¹•ä¸‹æ–¹50åƒç´ å°±åˆ é™¤
    
    def get_position(self):
        return int(self.x - self.width//2), int(self.y), int(self.width), int(self.height)

# å¢å¼ºçš„äººè„¸æ£€æµ‹å›è°ƒå‡½æ•°ï¼ˆå¸¦æ‰è½æ•ˆæœï¼‰
def face_detection_callback(frame):
    global falling_faces, last_face_capture_time
    import time
    
    img = frame.to_ndarray(format="bgr24")
    frame_height, frame_width = img.shape[:2]
    current_time = time.time()
    
    if face_detection_settings['enabled']:
        try:
            # åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨
            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            
            # æ£€æµ‹äººè„¸
            faces = face_cascade.detectMultiScale(
                gray,
                scaleFactor=1.1,
                minNeighbors=5,
                minSize=(30, 30)
            )
            
            # æ›´æ–°äººè„¸æ•°é‡ç»Ÿè®¡
            try:
                st.session_state.face_count = len(faces)
            except:
                pass
            
            # ç»˜åˆ¶äººè„¸æ¡†å¹¶æ•è·äººè„¸ï¼ˆæ¯1ç§’ä¸€æ¬¡ï¼‰
            if faces is not None and len(faces) > 0:
                for (x, y, w, h) in faces:
                    # ç»˜åˆ¶æ£€æµ‹æ¡†
                    cv2.rectangle(img, (x, y), (x + w, y + h), face_detection_settings['color'], 2)
                    cv2.putText(img, 'Face', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, face_detection_settings['color'], 1)
                    
                    # æ¯1ç§’æ•è·ä¸€æ¬¡äººè„¸ç”¨äºæ‰è½æ•ˆæœ
                    if (face_detection_settings['falling_effect'] and 
                        current_time - last_face_capture_time > 1.0):
                        
                        # æå–äººè„¸åŒºåŸŸ
                        face_roi = img[y:y+h, x:x+w].copy()
                        
                        # è°ƒæ•´äººè„¸å¤§å°ï¼ˆå˜å°ä¸€ç‚¹ç”¨äºæ‰è½ï¼‰
                        face_size = min(w, h, 60)  # æœ€å¤§60åƒç´ 
                        if face_size > 20:  # æœ€å°20åƒç´ 
                            face_roi_resized = cv2.resize(face_roi, (face_size, face_size))
                            
                            # åˆ›å»ºæ–°çš„æ‰è½äººè„¸å¯¹è±¡
                            new_falling_face = FallingFace(
                                face_roi_resized, 
                                x, 
                                frame_width, 
                                frame_height
                            )
                            falling_faces.append(new_falling_face)
                            
                            # é™åˆ¶åŒæ—¶æ‰è½çš„äººè„¸æ•°é‡
                            if len(falling_faces) > 10:
                                falling_faces = falling_faces[-10:]
                        
                        last_face_capture_time = current_time
                        break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„äººè„¸
        
        except Exception as e:
            # å¦‚æœæ£€æµ‹å¤±è´¥ï¼Œè‡³å°‘è¿”å›åŸå›¾åƒ
            pass
    
    # æ›´æ–°å’Œç»˜åˆ¶æ‰è½çš„äººè„¸
    if face_detection_settings['falling_effect']:
        try:
            # æ›´æ–°æ‰è½äººè„¸ä½ç½®
            active_faces = []
            for falling_face in falling_faces:
                if falling_face.update():  # å¦‚æœè¿˜åœ¨å±å¹•å†…
                    active_faces.append(falling_face)
                    
                    # åœ¨å›¾åƒä¸Šç»˜åˆ¶æ‰è½çš„äººè„¸
                    fx, fy, fw, fh = falling_face.get_position()
                    
                    # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    if (fx >= 0 and fy >= 0 and 
                        fx + fw <= frame_width and 
                        fy + fh <= frame_height):
                        
                        # ç®€å•åœ°å åŠ äººè„¸å›¾åƒï¼ˆä¸åšæ—‹è½¬ï¼Œä¿æŒæ€§èƒ½ï¼‰
                        img[fy:fy+fh, fx:fx+fw] = falling_face.face_img
                        
                        # å¯é€‰ï¼šæ·»åŠ ä¸€ä¸ªé€æ˜è¾¹æ¡†æ•ˆæœ
                        cv2.rectangle(img, (fx-1, fy-1), (fx+fw+1, fy+fh+1), (255, 255, 255), 1)
            
            falling_faces = active_faces
            
        except Exception as e:
            # å¦‚æœæ‰è½æ•ˆæœå‡ºé”™ï¼Œæ¸…ç©ºæ‰è½åˆ—è¡¨
            falling_faces = []
    
    return av.VideoFrame.from_ndarray(img, format="bgr24")

# é¡µé¢é…ç½®
st.set_page_config(
    page_title=get_text('page_title', st.session_state.language),
    page_icon="ğŸ¨",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ–æ¨¡å‹
@st.cache_resource
def load_model():
    pipe = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
    pipe.to("cuda" if torch.cuda.is_available() else "cpu")
    return pipe

def main():
    # å·¦ä¾§è¾¹æ 
    with st.sidebar:
        # è¯­è¨€åˆ‡æ¢æŒ‰é’®
        st.subheader(get_text('language_switch', st.session_state.language))
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button(get_text('lang_zh', st.session_state.language), use_container_width=True):
                st.session_state.language = 'zh'
                st.rerun()
                
        with col2:
            if st.button(get_text('lang_en', st.session_state.language), use_container_width=True):
                st.session_state.language = 'en'
                st.rerun()
        
        st.markdown("---")
        
        # é¡µé¢å¯¼èˆª
        st.subheader(get_text('navigation', st.session_state.language))
        
        # å›¾åƒç”Ÿæˆå™¨æŒ‰é’®
        if st.button(get_text('nav_image_gen', st.session_state.language), 
                    type="primary" if st.session_state.current_page == 'image_gen' else "secondary",
                    use_container_width=True):
            st.session_state.current_page = 'image_gen'
            st.rerun()
        
        # æ‘„åƒå¤´è¯†åˆ«æŒ‰é’®
        if st.button(get_text('nav_camera', st.session_state.language),
                    type="primary" if st.session_state.current_page == 'camera' else "secondary", 
                    use_container_width=True):
            st.session_state.current_page = 'camera'
            st.rerun()
        
        st.markdown("---")
    
    # æ ¹æ®å½“å‰é¡µé¢æ˜¾ç¤ºå†…å®¹
    if st.session_state.current_page == 'image_gen':
        image_generator_page()
    elif st.session_state.current_page == 'camera':
        camera_page()

def image_generator_page():
    # å·¦ä¾§è¾¹æ  - å›¾åƒç”Ÿæˆè®¾ç½®
    with st.sidebar:
        
        # ç³»ç»Ÿä¿¡æ¯
        st.subheader(get_text('system_info', st.session_state.language))
        st.info(get_text('pytorch_info', st.session_state.language))
        device_info = get_text('device_gpu', st.session_state.language) if torch.cuda.is_available() else get_text('device_cpu', st.session_state.language)
        st.info(device_info)
        st.markdown("---")
        
        # è¾“å…¥å‚æ•°
        st.subheader(get_text('image_settings', st.session_state.language))
        prompt = st.text_area(
            get_text('image_description', st.session_state.language), 
            value=get_text('default_prompt', st.session_state.language),
            height=100,
            help=get_text('image_description_help', st.session_state.language)
        )
        
        steps = st.slider(get_text('generation_steps', st.session_state.language), min_value=10, max_value=50, value=20, help=get_text('steps_help', st.session_state.language))
        
        # å›¾åƒå°ºå¯¸é€‰æ‹©
        st.subheader(get_text('image_dimensions', st.session_state.language))
        size_options = {
            get_text('size_512x512', st.session_state.language): (512, 512),
            get_text('size_768x512', st.session_state.language): (768, 512),
            get_text('size_512x768', st.session_state.language): (512, 768)
        }
        selected_size = st.selectbox(get_text('select_size', st.session_state.language), list(size_options.keys()))
        width, height = size_options[selected_size]
        
        st.markdown("---")
        
        # é¢„è®¾æç¤ºè¯
        st.subheader(get_text('quick_select', st.session_state.language))
        preset_prompts = PRESET_PROMPTS[st.session_state.language]
        
        for name, preset_prompt in preset_prompts.items():
            if st.button(name, use_container_width=True):
                st.session_state.selected_prompt = preset_prompt
        
        # æ›´æ–°æç¤ºè¯
        if 'selected_prompt' in st.session_state:
            prompt = st.session_state.selected_prompt
            # æ¸…é™¤session stateä»¥é¿å…æŒç»­æ›´æ–°
            del st.session_state.selected_prompt
        
        st.markdown("---")
        
        # ç”ŸæˆæŒ‰é’®
        generate_button = st.button(get_text('generate_button', st.session_state.language), type="primary", use_container_width=True)
    
    # ä¸»å†…å®¹åŒºåŸŸ
    st.title(get_text('image_display', st.session_state.language))
    
    # åˆ›å»ºä¸¤åˆ—å¸ƒå±€ç”¨äºæ˜¾ç¤ºç»“æœ
    col1, col2 = st.columns([1, 1])
    
    # åˆå§‹åŒ–session state
    if 'generated_images' not in st.session_state:
        st.session_state.generated_images = []
    if 'generation_history' not in st.session_state:
        st.session_state.generation_history = []
    
    # å¤„ç†ç”ŸæˆæŒ‰é’®ç‚¹å‡»
    if generate_button and prompt.strip():
        with st.spinner(get_text('generating', st.session_state.language)):
            try:
                # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
                progress_text = st.empty()
                progress_bar = st.progress(0)
                
                progress_text.text(get_text('loading_model', st.session_state.language))
                progress_bar.progress(20)
                pipe = load_model()
                
                progress_text.text(get_text('generating_image', st.session_state.language))
                progress_bar.progress(50)
                
                # ç”Ÿæˆå›¾åƒ
                image = pipe(prompt, num_inference_steps=steps, width=width, height=height).images[0]
                
                progress_text.text(get_text('generation_complete', st.session_state.language))
                progress_bar.progress(100)
                time.sleep(0.5)  # çŸ­æš‚æ˜¾ç¤ºå®ŒæˆçŠ¶æ€
                
                # æ¸…é™¤è¿›åº¦æ˜¾ç¤º
                progress_text.empty()
                progress_bar.empty()
                
                # ä¿å­˜åˆ°session state
                timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
                st.session_state.generated_images.insert(0, {
                    'image': image,
                    'prompt': prompt,
                    'steps': steps,
                    'size': f"{width}x{height}",
                    'timestamp': timestamp
                })
                
                # ä¿æŒæœ€å¤š10å¼ å›¾åƒ
                if len(st.session_state.generated_images) > 10:
                    st.session_state.generated_images = st.session_state.generated_images[:10]
                
                st.success(get_text('generation_success', st.session_state.language))
                
            except Exception as e:
                st.error(get_text('generation_failed', st.session_state.language) + str(e))
    
    # æ˜¾ç¤ºç”Ÿæˆçš„å›¾åƒ
    if st.session_state.generated_images:
        st.subheader(get_text('latest_image', st.session_state.language))
        
        # æ˜¾ç¤ºæœ€æ–°å›¾åƒ
        latest_image = st.session_state.generated_images[0]
        
        with col1:
            st.image(
                latest_image['image'], 
                caption=get_text('prompt_label', st.session_state.language) + latest_image['prompt'][:50] + "...", 
                use_container_width=True
            )
            
            # å›¾åƒä¿¡æ¯
            st.info(f"""
            {get_text('prompt_info', st.session_state.language)}{latest_image['prompt']}
            {get_text('steps_info', st.session_state.language)}{latest_image['steps']}
            {get_text('size_info', st.session_state.language)}{latest_image['size']}
            {get_text('time_info', st.session_state.language)}{latest_image['timestamp']}
            """)
        
        with col2:
            # ä¸‹è½½æŒ‰é’®
            img_buffer = BytesIO()
            latest_image['image'].save(img_buffer, format='PNG')
            img_buffer.seek(0)
            
            st.download_button(
                label=get_text('download_button', st.session_state.language),
                data=img_buffer.getvalue(),
                file_name=f"ai_generated_{int(time.time())}.png",
                mime="image/png",
                use_container_width=True
            )
            
            # é‡æ–°ç”ŸæˆæŒ‰é’®
            if st.button(get_text('regenerate_button', st.session_state.language), use_container_width=True):
                st.rerun()
            
            # æ¸…é™¤å†å²æŒ‰é’®
            if st.button(get_text('clear_history', st.session_state.language), use_container_width=True):
                st.session_state.generated_images = []
                st.rerun()
        
        # å†å²å›¾åƒå±•ç¤º
        if len(st.session_state.generated_images) > 1:
            st.subheader(get_text('history_title', st.session_state.language))
            
            # ä½¿ç”¨ç½‘æ ¼å¸ƒå±€æ˜¾ç¤ºå†å²å›¾åƒ
            cols_per_row = 3
            for i in range(1, len(st.session_state.generated_images)):
                if (i - 1) % cols_per_row == 0:
                    cols = st.columns(cols_per_row)
                
                with cols[(i - 1) % cols_per_row]:
                    img_data = st.session_state.generated_images[i]
                    st.image(img_data['image'], caption=f"{img_data['timestamp']}", use_container_width=True)
                    
                    if st.button(get_text('view_details', st.session_state.language) + str(i), key=f"detail_{i}"):
                        st.session_state.selected_detail = i
    
    else:
        # ç©ºçŠ¶æ€æ˜¾ç¤º
        st.info(get_text('empty_state', st.session_state.language))
        
        # æ˜¾ç¤ºç¤ºä¾‹å›¾åƒåŒºåŸŸ
        st.subheader(get_text('features_title', st.session_state.language))
        feature_cols = st.columns(3)
        
        with feature_cols[0]:
            st.markdown(get_text('feature_quality', st.session_state.language))
        
        with feature_cols[1]:
            st.markdown(get_text('feature_gpu', st.session_state.language))
        
        with feature_cols[2]:
            st.markdown(get_text('feature_ui', st.session_state.language))

def camera_page():
    # å·¦ä¾§è¾¹æ  - æ‘„åƒå¤´è®¾ç½®
    with st.sidebar:
        st.title(get_text('page_camera', st.session_state.language))
        st.markdown("---")
        
        # æ‘„åƒå¤´è®¾ç½®
        st.subheader("ğŸ“¹ " + get_text('camera_settings', st.session_state.language) if st.session_state.language == 'zh' else "ğŸ“¹ Camera Settings")
        
        # äººè„¸æ£€æµ‹å¼€å…³
        face_detection_enabled = st.checkbox(
            get_text('face_detection', st.session_state.language) if st.session_state.language == 'zh' else "Face Detection",
            value=True,
            help=get_text('face_detection_help', st.session_state.language) if st.session_state.language == 'zh' else "Enable to mark detected faces in video"
        )
        
        # äººè„¸æ‰è½æ•ˆæœå¼€å…³
        falling_effect_enabled = st.checkbox(
            "ğŸ­ äººè„¸æ‰è½æ•ˆæœ" if st.session_state.language == 'zh' else "ğŸ­ Face Falling Effect",
            value=True,
            help="å¯ç”¨åæ£€æµ‹åˆ°çš„äººè„¸ä¼šä»é¡¶éƒ¨æ‰è½" if st.session_state.language == 'zh' else "Detected faces will fall from top when enabled"
        )
        
        st.markdown("---")
        
        # æ£€æµ‹è®¾ç½®
        st.subheader("ğŸ” " + ("æ£€æµ‹è®¾ç½®" if st.session_state.language == 'zh' else "Detection Settings"))
        
        # ç½®ä¿¡åº¦é˜ˆå€¼
        confidence_threshold = st.slider(
            "æ£€æµ‹ç½®ä¿¡åº¦" if st.session_state.language == 'zh' else "Detection Confidence",
            min_value=0.1,
            max_value=1.0,
            value=0.3,
            step=0.05,
            help="ç½®ä¿¡åº¦è¶Šé«˜ï¼Œæ£€æµ‹è¶Šä¸¥æ ¼" if st.session_state.language == 'zh' else "Higher confidence means stricter detection"
        )
        
        # æ£€æµ‹æ¡†é¢œè‰²é€‰æ‹©
        color_option = st.selectbox(
            "æ£€æµ‹æ¡†é¢œè‰²" if st.session_state.language == 'zh' else "Detection Box Color",
            ["Green", "Red", "Blue", "Yellow", "Purple"],
            index=0
        )
        
        color_map = {
            "Green": (0, 255, 0),
            "Red": (0, 0, 255),
            "Blue": (255, 0, 0),
            "Yellow": (0, 255, 255),
            "Purple": (255, 0, 255)
        }
        detection_color = color_map[color_option]
        
        # æ‰è½æ•ˆæœè®¾ç½®
        if falling_effect_enabled:
            st.subheader("ğŸ­ " + ("æ‰è½æ•ˆæœè®¾ç½®" if st.session_state.language == 'zh' else "Falling Effect Settings"))
            
            # æ‰è½é€Ÿåº¦æ§åˆ¶
            falling_speed = st.slider(
                "æ‰è½é€Ÿåº¦" if st.session_state.language == 'zh' else "Falling Speed",
                min_value=1.0,
                max_value=10.0,
                value=3.0,
                step=0.5,
                help="è°ƒæ•´äººè„¸æ‰è½çš„é€Ÿåº¦" if st.session_state.language == 'zh' else "Adjust the speed of falling faces"
            )
        else:
            falling_speed = 3.0
        
        st.markdown("---")
        
        # æ‘„åƒå¤´çŠ¶æ€
        st.subheader("ğŸ“Š " + ("æ‘„åƒå¤´çŠ¶æ€" if st.session_state.language == 'zh' else "Camera Status"))
        
        if 'webrtc_ctx' in st.session_state and st.session_state.webrtc_ctx:
            if st.session_state.webrtc_ctx.state.playing:
                st.success("ğŸŸ¡ " + ("è¿è¡Œä¸­" if st.session_state.language == 'zh' else "Running"))
            else:
                st.info("ğŸŸ¢ " + ("å°±ç»ª" if st.session_state.language == 'zh' else "Ready"))
        else:
            st.warning("ğŸ”´ " + ("å·²åœæ­¢" if st.session_state.language == 'zh' else "Stopped"))
        
        st.markdown("---")
        
        # æ£€æµ‹ç»Ÿè®¡
        st.subheader("ğŸ“ˆ " + ("æ£€æµ‹ç»Ÿè®¡" if st.session_state.language == 'zh' else "Detection Stats"))
        
        col_faces, col_fps = st.columns(2)
        with col_faces:
            st.metric(
                "æ£€æµ‹åˆ°çš„äººè„¸" if st.session_state.language == 'zh' else "Faces Detected",
                st.session_state.face_count
            )
        
        with col_fps:
            st.metric(
                "å¤„ç†å¸§ç‡" if st.session_state.language == 'zh' else "Processing FPS",
                f"{st.session_state.processing_fps}"
            )
    
    # ä¸»å†…å®¹åŒºåŸŸ
    st.title("ğŸ“¹ " + ("AIæ‘„åƒå¤´äººè„¸è¯†åˆ«" if st.session_state.language == 'zh' else "AI Camera Face Detection"))
    
    # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # æ›´æ–°è®¾ç½®
        face_detection_settings['enabled'] = face_detection_enabled
        face_detection_settings['color'] = detection_color
        face_detection_settings['confidence'] = confidence_threshold
        face_detection_settings['falling_effect'] = falling_effect_enabled
        face_detection_settings['falling_speed'] = falling_speed
        
        # WebRTCæ‘„åƒå¤´æµ
        webrtc_ctx = webrtc_streamer(
            key="face-detection",
            video_frame_callback=face_detection_callback,
            rtc_configuration=RTC_CONFIGURATION,
            media_stream_constraints={"video": True, "audio": False},
        )
        
        # å­˜å‚¨webrtc contextåˆ°session state
        st.session_state.webrtc_ctx = webrtc_ctx
    
    with col2:
        # ä½¿ç”¨è¯´æ˜
        st.subheader("ğŸ“‹ " + ("ä½¿ç”¨è¯´æ˜" if st.session_state.language == 'zh' else "Instructions"))
        
        if st.session_state.language == 'zh':
            st.markdown("""
            1. ç‚¹å‡»"START"æŒ‰é’®å¯åŠ¨æ‘„åƒå¤´
            
            2. è°ƒæ•´æ£€æµ‹è®¾ç½®æ¥ä¼˜åŒ–è¯†åˆ«æ•ˆæœ
            
            3. ç»¿è‰²æ¡†è¡¨ç¤ºæ£€æµ‹åˆ°çš„äººè„¸
            
            4. ğŸ­ å¼€å¯æ‰è½æ•ˆæœï¼Œäººè„¸ä¼šæ¯ç§’å¤åˆ¶å¹¶æ‰è½
            
            5. å¯ä»¥è°ƒæ•´æ‰è½é€Ÿåº¦å’Œæ£€æµ‹çµæ•åº¦
            """)
            
            st.info("ğŸ”’ éšç§æé†’ï¼šè§†é¢‘æµä»…åœ¨æœ¬åœ°å¤„ç†ï¼Œä¸ä¼šä¸Šä¼ åˆ°æœåŠ¡å™¨")
        else:
            st.markdown("""
            1. Click "START" button to launch camera
            
            2. Adjust detection settings to optimize recognition
            
            3. Green boxes indicate detected faces
            
            4. ğŸ­ Enable falling effect to make faces fall every second
            
            5. Adjust falling speed and detection sensitivity
            """)
            
            st.info("ğŸ”’ Privacy Notice: Video stream is processed locally only, not uploaded to server")
        
        # å®æ—¶ç»Ÿè®¡æ˜¾ç¤º
        if face_detection_enabled:
            status_text = "äººè„¸æ£€æµ‹: å¼€å¯" if st.session_state.language == 'zh' else "Face Detection: ON"
            st.success(f"âœ… {status_text}")
        else:
            status_text = "äººè„¸æ£€æµ‹: å…³é—­" if st.session_state.language == 'zh' else "Face Detection: OFF"
            st.warning(f"âš ï¸ {status_text}")
            
        if falling_effect_enabled:
            falling_text = "æ‰è½æ•ˆæœ: å¼€å¯" if st.session_state.language == 'zh' else "Falling Effect: ON"
            st.success(f"ğŸ­ {falling_text}")
            
            # æ˜¾ç¤ºå½“å‰æ‰è½ä¸­çš„äººè„¸æ•°é‡
            try:
                falling_count = len(falling_faces) if 'falling_faces' in globals() else 0
                count_text = f"æ‰è½ä¸­: {falling_count} ä¸ªäººè„¸" if st.session_state.language == 'zh' else f"Falling: {falling_count} faces"
                st.info(f"ğŸ“ˆ {count_text}")
            except:
                pass
        else:
            falling_text = "æ‰è½æ•ˆæœ: å…³é—­" if st.session_state.language == 'zh' else "Falling Effect: OFF"
            st.info(f"ğŸ­ {falling_text}")

if __name__ == "__main__":
    main()